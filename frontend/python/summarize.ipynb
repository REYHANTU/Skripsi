{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized data has been saved to ./preprocessed/testing.csv\n",
      "                                             Summary\n",
      "0  Bitcoin (BTC) baru-baru ini mencapai titik ter...\n",
      "1  Lonjakan nilai koin ini telah memungkinkan ban...\n",
      "2  Institut Keuangan Korsel, yang diwakili oleh p...\n",
      "3  Peringkat ini menyoroti kehadiran signifikan D...\n",
      "4  Harga Ripple (XRP) saat ini mengalami trenbear...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract sentence embedding using BERT\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.detach().numpy().flatten()  # Flatten to remove extra dimensions\n",
    "\n",
    "# Function for extractive summarization\n",
    "def extractive_summary(text, ratio=0.7):\n",
    "    sentences = text.split('. ')\n",
    "    sentence_embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "    # Compute cosine similarity between sentences\n",
    "    similarities = cosine_similarity(sentence_embeddings)\n",
    "\n",
    "    # Sum the similarities for each sentence\n",
    "    sentence_scores = similarities.sum(axis=1)\n",
    "\n",
    "    # Determine the number of sentences to select based on the ratio\n",
    "    top_n = max(1, int(len(sentences) * ratio))\n",
    "\n",
    "    # Select sentences with the highest scores\n",
    "    top_sentence_indices = np.argsort(sentence_scores)[-top_n:]\n",
    "    top_sentence_indices.sort()  # Sort sentences by their original position\n",
    "\n",
    "    # Join the selected sentences to form the summary\n",
    "    summary = '. '.join([sentences[i] for i in top_sentence_indices])\n",
    "    return summary\n",
    "\n",
    "# Read data from CSV file\n",
    "csv_input_file = './Datasets/combined_datasets.csv'  # Path to your input CSV file\n",
    "csv_output_file = './preprocessed/testing.csv'  # Path to save the summarized output\n",
    "df = pd.read_csv(csv_input_file)\n",
    "\n",
    "# Ensure that the 3rd column (index 2) contains the content to be summarized\n",
    "# If the column name is different, adjust the column index accordingly\n",
    "\n",
    "summaries = []\n",
    "\n",
    "# Loop through each row and summarize the content in the 3rd column\n",
    "for _, row in df.iterrows():\n",
    "    content = row.iloc[2]  # Adjust if your content column is not in the 3rd position\n",
    "    summary = extractive_summary(content, ratio=0.7)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Add the summaries as a new column to the dataframe\n",
    "df['Summary'] = summaries\n",
    "\n",
    "# Save the summarized data to a new CSV file\n",
    "df.to_csv(csv_output_file, index=False)\n",
    "\n",
    "# Print the summarized data\n",
    "print(f\"Summarized data has been saved to {csv_output_file}\")\n",
    "print(df[['Summary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Reyhan\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\Reyhan\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52457b907df54e7bb4137ebb51c60c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Reyhan\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Reyhan\\.cache\\huggingface\\hub\\models--indolem--indobert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e533831f145b46e78600763136f5fde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436e550c2ce64c5a91ab383f2679d77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4372682274c4ddf98cd1f5961f9c440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3044313531594379bc1bac6003dce970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b5db418e5245bc8da67a08e91f23a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized data has been saved to ./preprocessed/testing(2).csv\n",
      "                                               Title          Date  \\\n",
      "0  Michael Saylor Tetap Optimis Meski Harga BTC M...  23 June 2024   \n",
      "1  Masa Depan Toncoin: Harga TON di Ambang Kebang...  23 June 2024   \n",
      "2  Think Tank Korea Selatan Ragukan ETF Bitcoin, ...  24 June 2024   \n",
      "3  Dogwifhat (WIF) Keluar dari 50 Besar, Harga An...  24 June 2024   \n",
      "4  XRP Ripple Terus Berkonsolidasi, Kapan Harga A...  24 June 2024   \n",
      "\n",
      "                                             Summary  \n",
      "0  Bitcoin (BTC) baru-baru ini mencapai titik ter...  \n",
      "1  Lonjakan nilai koin ini telah memungkinkan ban...  \n",
      "2  Institut Keuangan Korsel, yang diwakili oleh p...  \n",
      "3  Dalam peristiwa dramatis,memecoinberbasis Sola...  \n",
      "4  Harga Ripple (XRP) saat ini mengalami trenbear...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract sentence embedding using BERT\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.detach().numpy().flatten()  # Flatten to remove extra dimensions\n",
    "\n",
    "# Function for extractive summarization\n",
    "def extractive_summary(text, ratio=0.7):\n",
    "    sentences = text.split('. ')\n",
    "    sentence_embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "    # Compute cosine similarity between sentences\n",
    "    similarities = cosine_similarity(sentence_embeddings)\n",
    "\n",
    "    # Sum the similarities for each sentence\n",
    "    sentence_scores = similarities.sum(axis=1)\n",
    "\n",
    "    # Determine the number of sentences to select based on the ratio\n",
    "    top_n = max(1, int(len(sentences) * ratio))\n",
    "\n",
    "    # Select sentences with the highest scores\n",
    "    top_sentence_indices = np.argsort(sentence_scores)[-top_n:]\n",
    "    top_sentence_indices.sort()  # Sort sentences by their original position\n",
    "\n",
    "    # Join the selected sentences to form the summary\n",
    "    summary = '. '.join([sentences[i] for i in top_sentence_indices])\n",
    "    return summary\n",
    "\n",
    "# Read data from CSV file\n",
    "csv_input_file = './Datasets/combined_datasets.csv'  # Path to your input CSV file\n",
    "csv_output_file = './preprocessed/testing(2).csv'  # Path to save the summarized output\n",
    "df = pd.read_csv(csv_input_file)\n",
    "\n",
    "# Ensure that the columns for 'title', 'date', and 'content' are correctly identified\n",
    "# Adjust the column names if they are different in your dataset\n",
    "title_column = 'Title'   # Column name for title\n",
    "date_column = 'Date'  # Column name for date\n",
    "content_column = 'Summary'  # Column name for content to be summarized\n",
    "\n",
    "summaries = []\n",
    "\n",
    "# Loop through each row and summarize the content\n",
    "for _, row in df.iterrows():\n",
    "    content = row[content_column]\n",
    "    summary = extractive_summary(content, ratio=0.7)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Add the summaries and retain title and date columns in the new DataFrame\n",
    "df_summary = df[[title_column, date_column]].copy()\n",
    "df_summary['Summary'] = summaries\n",
    "\n",
    "# Save the summarized data to a new CSV file\n",
    "df_summary.to_csv(csv_output_file, index=False)\n",
    "\n",
    "# Print the summarized data\n",
    "print(f\"Summarized data has been saved to {csv_output_file}\")\n",
    "print(df_summary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
