{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized data has been saved to ./preprocessed/testing.csv\n",
      "                                             Summary\n",
      "0  Bitcoin (BTC) baru-baru ini mencapai titik ter...\n",
      "1  Lonjakan nilai koin ini telah memungkinkan ban...\n",
      "2  Institut Keuangan Korsel, yang diwakili oleh p...\n",
      "3  Peringkat ini menyoroti kehadiran signifikan D...\n",
      "4  Harga Ripple (XRP) saat ini mengalami trenbear...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract sentence embedding using BERT\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.detach().numpy().flatten()  # Flatten to remove extra dimensions\n",
    "\n",
    "# Function for extractive summarization\n",
    "def extractive_summary(text, ratio=0.7):\n",
    "    sentences = text.split('. ')\n",
    "    sentence_embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "    # Compute cosine similarity between sentences\n",
    "    similarities = cosine_similarity(sentence_embeddings)\n",
    "\n",
    "    # Sum the similarities for each sentence\n",
    "    sentence_scores = similarities.sum(axis=1)\n",
    "\n",
    "    # Determine the number of sentences to select based on the ratio\n",
    "    top_n = max(1, int(len(sentences) * ratio))\n",
    "\n",
    "    # Select sentences with the highest scores\n",
    "    top_sentence_indices = np.argsort(sentence_scores)[-top_n:]\n",
    "    top_sentence_indices.sort()  # Sort sentences by their original position\n",
    "\n",
    "    # Join the selected sentences to form the summary\n",
    "    summary = '. '.join([sentences[i] for i in top_sentence_indices])\n",
    "    return summary\n",
    "\n",
    "# Read data from CSV file\n",
    "csv_input_file = './Datasets/combined_datasets.csv'  # Path to your input CSV file\n",
    "csv_output_file = './preprocessed/testing.csv'  # Path to save the summarized output\n",
    "df = pd.read_csv(csv_input_file)\n",
    "\n",
    "# Ensure that the 3rd column (index 2) contains the content to be summarized\n",
    "# If the column name is different, adjust the column index accordingly\n",
    "\n",
    "summaries = []\n",
    "\n",
    "# Loop through each row and summarize the content in the 3rd column\n",
    "for _, row in df.iterrows():\n",
    "    content = row.iloc[2]  # Adjust if your content column is not in the 3rd position\n",
    "    summary = extractive_summary(content, ratio=0.7)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Add the summaries as a new column to the dataframe\n",
    "df['Summary'] = summaries\n",
    "\n",
    "# Save the summarized data to a new CSV file\n",
    "df.to_csv(csv_output_file, index=False)\n",
    "\n",
    "# Print the summarized data\n",
    "print(f\"Summarized data has been saved to {csv_output_file}\")\n",
    "print(df[['Summary']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
